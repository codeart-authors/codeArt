import random
import torch


class DataCollatorForCodeArt(object):

    def __init__(self, tokenizer, label2id, max_functions) -> None:
        self.tokenizer = tokenizer
        self.label2id = label2id
        self.max_functions = max_functions

    def __call__(
        self,
        examples
    ):
        batch = {
            'input_ids': [],
            'attention_mask': [],
            'relative_position_matrix': [],
            'labels': [],
            'sequence_mask': [],
            'all_labels': []
        }
        
        for example in examples:
            num_functions = 0
            sequence_mask = []
            for function in eval(example['functions'])[:self.max_functions]:
                encoded = self.tokenizer.inst_encode(function['code'], function['data_dep'])
                batch['input_ids'].append(encoded['input_ids'])
                batch['attention_mask'].append(encoded['attention_mask'])
                batch['relative_position_matrix'].append(encoded['relative_position_matrix'])
                num_functions += 1
                sequence_mask.append(1)
            for _ in range(num_functions, self.max_functions):  # pad to max_functions
                encoded = self.tokenizer.inst_encode([], [])
                batch['input_ids'].append(encoded['input_ids'])
                batch['attention_mask'].append(encoded['attention_mask'])
                batch['relative_position_matrix'].append(encoded['relative_position_matrix'])
                sequence_mask.append(0)
            batch["labels"].append(self.label2id[random.sample(example['labels'], 1)[0]])
            batch["sequence_mask"].append(sequence_mask)
            batch["all_labels"].append(example['labels'])
        return {
            'input_ids': torch.stack(batch['input_ids']),
            'attention_mask': torch.stack(batch['attention_mask']),
            'relative_position_matrix': torch.stack(batch['relative_position_matrix']),
            'labels': torch.tensor(batch['labels'], dtype=torch.long).unsqueeze(1),
            'sequence_mask': torch.tensor(batch['sequence_mask'], dtype=torch.long),
            'all_labels': batch['all_labels']
        }


class DataCollatorForCodeArtMultilabel(object):

    def __init__(self, tokenizer, label2id, max_functions) -> None:
        self.tokenizer = tokenizer
        self.label2id = label2id
        self.max_functions = max_functions

    def __call__(
        self,
        examples
    ):
        batch = {
            'input_ids': [],
            'attention_mask': [],
            'relative_position_matrix': [],
            'labels': [],
            'sequence_mask': [],
        }
        
        for example in examples:
            num_functions = 0
            sequence_mask = []
            for function in eval(example['functions'])[:self.max_functions]:
                encoded = self.tokenizer.inst_encode(function['code'], function['data_dep'])
                batch['input_ids'].append(encoded['input_ids'])
                batch['attention_mask'].append(encoded['attention_mask'])
                batch['relative_position_matrix'].append(encoded['relative_position_matrix'])
                num_functions += 1
                sequence_mask.append(1)
            for _ in range(num_functions, self.max_functions):  # pad to max_functions
                encoded = self.tokenizer.inst_encode([], [])
                batch['input_ids'].append(encoded['input_ids'])
                batch['attention_mask'].append(encoded['attention_mask'])
                batch['relative_position_matrix'].append(encoded['relative_position_matrix'])
                sequence_mask.append(0)
            labels = torch.zeros(len(self.label2id))
            for l in example['labels']:
                labels[self.label2id[l]] = 1
            batch['labels'].append(labels)
            batch["sequence_mask"].append(sequence_mask)
        return {
            'input_ids': torch.stack(batch['input_ids']),
            'attention_mask': torch.stack(batch['attention_mask']),
            'relative_position_matrix': torch.stack(batch['relative_position_matrix']),
            'labels': torch.stack(batch['labels']),
            'sequence_mask': torch.tensor(batch['sequence_mask'], dtype=torch.long),
        }